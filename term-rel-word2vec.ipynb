{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term relations with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will be analyzing movie discussions and create a trivial synonym engine. \n",
    "Our engine is based on [Word2Vec](https://en.wikipedia.org/wiki/Word2vec), a family of shallow, \n",
    "two-layer neural networks that are trained to reconstruct linguistic contexts of words.\n",
    "In essence, Word2Vec attempts to understand meaning and semantic relationships among words.\n",
    "\n",
    "We will be using the Spark machine learning package to implement our synonyms service. \n",
    "Spark Machine Learning comes in two flavours:\n",
    "\n",
    "* [SparkML](https://spark.apache.org/docs/latest/ml-guide.html) A Dataframe-based API\n",
    "* [Spark MLlib](https://spark.apache.org/docs/latest/mllib-guide.html) A lower level, older, RDD-based API\n",
    "\n",
    "Since Spark 2.0, MLlib is in maintenance mode, meaning that no new features are implemented for it. \n",
    "Therefore, for new projects, it should be avoided. Some features of MLlib are yet to be\n",
    "ported to SparkML, and the documentation is better for MLlib.\n",
    "\n",
    "For the remaining of the tutorial, we will be using the SparkML variant.\n",
    "\n",
    "The dataset we will be using comes from [Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial/data);\n",
    "the full dataset is available [at this location]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data as an RDD file. As the data contains HTML code, \n",
    "we need to clear it out. We also need to remove punctuation marks and lower case all our\n",
    "words. This will make our input vocabulary much smaller and therefore Word2Vec will not\n",
    "need to use too big vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://burcus-mbp.byod.tudelft.net:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1633685035151)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "path: String = datasets/imdb.csv\n",
       "data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at map at <console>:34\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = s\"datasets/imdb.csv\"\n",
    "\n",
    "val data = sc.textFile(path)\n",
    "    // Remove HTML, string escapes and punctuation\n",
    "    .map(w => w.replaceAll(\"\"\"<(?!\\/?a(?=>|\\s.*>))\\/?.*>\"\"\", \"\"))\n",
    "    .map(w => w.replaceAll(\"\"\"[\\…\\”\\'\\’\\`\\,\\(\\)\\\"\\\\]\"\"\", \"\"))\n",
    "    // Make lowercase\n",
    "    .map(w => w.toLowerCase)\n",
    "    // Word2Vec works at the sentence level\n",
    "    .flatMap(c => c.split(\"[.?!;:]\")).map(_.trim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what our raw data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  R:review\n",
      "  R:jennifer ehle was sparkling in pride and prejudice\n",
      "  R:jeremy northam was simply wonderful in the winslow boy\n"
     ]
    }
   ],
   "source": [
    "data.take(3).foreach(l => println(\"  R:\" + l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the data to a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since SparkML is based on Dataframes, we need to convert our source RDD to a suitable Dataframe.\n",
    "To do so, we first create a schema, consisting of a Sequence of fields that contain Arrays of Strings :-)\n",
    "\n",
    "Remember that Word2Vec treats text as a bag of words; a bag of word representation on a computer is an\n",
    "Array of Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WrappedArray(review)]\n",
      "[WrappedArray(jennifer, ehle, was, sparkling, in, pride, and, prejudice)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types._\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(text,ArrayType(StringType,true),true))\n",
       "documentDF: org.apache.spark.sql.DataFrame = [text: array<string>]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// Convert trainData from RDD[String] to DataFrame[Array[String]]\n",
    "val schema = StructType(Seq(StructField(\"text\", ArrayType(StringType, true), true)))\n",
    "var documentDF = spark.createDataFrame(data.map(r => org.apache.spark.sql.Row(r.split(\" \"))), schema)\n",
    "documentDF.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataframe above, we have lots of words that are repeating: think for example articles ('a', 'the'), prepositions (at, on, in) etc. Those words do not add much information to our dataset. You can get an\n",
    "intuitive understanding about this fact by trying to remove those words from everyday sentences: for example,\n",
    "\"a cat is under the table\" can be converted to \"cat is under table\" or even to \"cat is table\" and still get the idea.\n",
    "\n",
    "To increase the information density of our vectors, we can remove stopwords with `StopWordsRemover` transformer.\n",
    "We do so in a non-distructive manner; we add a new column in our Dataframe where the contents of our input text\n",
    "have been processed to remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WrappedArray(review),WrappedArray(review)]\n",
      "[WrappedArray(jennifer, ehle, was, sparkling, in, pride, and, prejudice),WrappedArray(jennifer, ehle, sparkling, pride, prejudice)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StopWordsRemover\n",
       "stopWordsRemover: org.apache.spark.ml.feature.StopWordsRemover = StopWordsRemover: uid=stopWords_45b04f7c3ac3, numStopWords=181, locale=en_GB, caseSensitive=false\n",
       "documentDF: org.apache.spark.sql.DataFrame = [text: array<string>, nostopwords: array<string>]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "// Remove stop words\n",
    "val stopWordsRemover = new StopWordsRemover().setInputCol(\"text\").setOutputCol(\"nostopwords\")\n",
    "documentDF = stopWordsRemover.transform(documentDF)\n",
    "\n",
    "documentDF.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our model!\n",
    "\n",
    "To exclude the long tail of words that do not appear frequently, we remove words will less than 10 appearences in our\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Word2Vec\n",
       "word2Vec: org.apache.spark.ml.feature.Word2Vec = w2v_01f05149e91a\n",
       "model: org.apache.spark.ml.feature.Word2VecModel = Word2VecModel: uid=w2v_01f05149e91a, numWords=21593, vectorSize=200\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Word2Vec\n",
    "\n",
    "// Learn a mapping from words to Vectors\n",
    "val word2Vec = new Word2Vec()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"result\")\n",
    "    .setVectorSize(200)\n",
    "    .setMinCount(10)\n",
    "val model = word2Vec.fit(documentDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for related terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the box, the Word2Vec API only allows us to check related for a single word. Let's give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[finney,0.7722606062889099]\n",
      "[perlman,0.7646912336349487]\n",
      "[dourif,0.746439516544342]\n",
      "[garrett,0.7226223945617676]\n",
      "[ford,0.6852742433547974]\n",
      "[palillo,0.6821386218070984]\n",
      "[pyun,0.6814386248588562]\n",
      "[bale,0.6788373589515686]\n",
      "[jeremy,0.678361177444458]\n",
      "[slater,0.6644403338432312]\n"
     ]
    }
   ],
   "source": [
    "// Find synonyms for a single word\n",
    "model.findSynonyms(\"pitt\", 10).collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see is that Word2Vec actually managed to uncover some related terms given a popular name in the dataset. What is more interesting however, is to see whether we can extract meaningfull terms with respect to a provided phrase. For this, we need to use Word2Vec's `findSynonyms(s: Vector)` function. \n",
    "\n",
    "To do so, we first define a function `toDF` that converts an input string to a vector representation suitable for searching; this basically just tokenizes an input string and converts it to a Spark Dataframe (hence the name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WrappedArray(james, bond)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "toDF: (s: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def toDF(s: String) = \n",
    "    spark.createDataFrame(Seq(s.trim\n",
    "        .toLowerCase\n",
    "        .split(\" \")\n",
    "    ).map(Tuple1.apply))\n",
    "    .toDF(\"text\")\n",
    "\n",
    "toDF(\"James Bond\").collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call the `transform` method on the created Dataframe; this converts our Dataframe to a vector representation using the same vocabulary as our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- result: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "q: org.apache.spark.sql.DataFrame = [text: array<string>, result: vector]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val q = model.transform(toDF(\"James Bond\")) \n",
    "q.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To automate the steps above, we create a method that takes a query (as String) and prints the 10 most relevant terms in our model, excluding terms that are included in the query itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query: (s: String)Unit\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query(s: String) = {\n",
    "    val q = model.transform(toDF(s))\n",
    "    val qTokens = s.toLowerCase.split(\" \")\n",
    "\n",
    "    model.findSynonyms(q.first.getAs[Vector](\"result\"), 10)\n",
    "        .filter(r => !qTokens.contains(r(0)))\n",
    "        .collect\n",
    "        .foreach(println)\n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[film,0.8398674130439758]\n",
      "[flick,0.598155677318573]\n",
      "[cartoon,0.5386905670166016]\n",
      "[mini-series,0.535006582736969]\n",
      "[moviesbut,0.5239917635917664]\n",
      "[documentary,0.5068673491477966]\n",
      "[show,0.5063693523406982]\n",
      "[stinker,0.49897581338882446]\n",
      "[picture,0.49263861775398254]\n"
     ]
    }
   ],
   "source": [
    "query(\"Movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dourif,0.6699931025505066]\n",
      "[terence,0.6649662256240845]\n",
      "[mabius,0.659092128276825]\n",
      "[dustin,0.6457685232162476]\n",
      "[stoltz,0.6436259746551514]\n",
      "[silva,0.6400138139724731]\n",
      "[raoul,0.6366493105888367]\n",
      "[elisabeth,0.6356052160263062]\n"
     ]
    }
   ],
   "source": [
    "query(\"brad pitt is a great actor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the nice side effects of being able to uncover latent meanings with tools like Word2Vec is being able to\n",
    "solve analogy problems. In the original Word2Vec paper, the authors show that, when trained on a sufficiently large corpus (billions of items), Word2Vec models can uncover relationships such as:\n",
    "\n",
    "`v(king) - v(man) + v(woman) =~ v(queen)`\n",
    "\n",
    "or, otherwise put: Man is to a king what Woman is to a queen (i.e. their gender). This works simply by performing algebraic vector operations on transformed vector reprensetations of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether our model can uncover such relationships as well, we first implement a few simple vector operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.DenseVector\n",
       "import math._\n",
       "vectorDiff: (xs: org.apache.spark.ml.linalg.Vector, ys: org.apache.spark.ml.linalg.Vector)org.apache.spark.ml.linalg.Vector\n",
       "vectorDistance: (xs: org.apache.spark.ml.linalg.Vector, ys: org.apache.spark.ml.linalg.Vector)Double\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.DenseVector\n",
    "import math._\n",
    "\n",
    "def vectorDiff(xs: Vector, ys: Vector) : Vector =\n",
    "    new DenseVector((xs.toArray zip ys.toArray).map { case (x,y) => x - y})\n",
    "\n",
    "def vectorDistance(xs: Vector, ys: Vector) = \n",
    "  sqrt((xs.toArray zip ys.toArray).map { case (x,y) => pow(y - x, 2) }.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we implement our analogy function; it returns the Euclidean distance between the vector differences between the entered terms as pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "analogy: (x: String, isToY: String, likeZ: String, isToA: String)Unit\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analogy(x: String, isToY: String, likeZ: String, isToA: String) {\n",
    "    val q = model.transform(toDF(x))\n",
    "    val w = model.transform(toDF(isToY))\n",
    "    val m = model.transform(toDF(likeZ))\n",
    "    val k = model.transform(toDF(isToA))\n",
    "\n",
    "    val left = vectorDiff(q.first.getAs[Vector](\"result\"), w.first.getAs[Vector](\"result\"))\n",
    "    val right = vectorDiff(k.first.getAs[Vector](\"result\"), m.first.getAs[Vector](\"result\"))\n",
    "    println(vectorDistance(left, right))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.502840631713733\n"
     ]
    }
   ],
   "source": [
    "analogy(\"king\",\"man\",\"queen\",\"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1463648185567026\n"
     ]
    }
   ],
   "source": [
    "analogy(\"soldier\",\"army\",\"sailor\",\"navy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1083978795843925\n"
     ]
    }
   ],
   "source": [
    "analogy(\"Athens\",\"Greece\",\"Paris\",\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8668779419101222\n"
     ]
    }
   ],
   "source": [
    "analogy(\"brother\",\"sister\",\"grandson\",\"grandaughter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.644923423629974\n"
     ]
    }
   ],
   "source": [
    "// The dataset is from the mid-00s :-)\n",
    "analogy(\"brad pitt\",\"angelina jolie\",\"Leonardo DiCaprio\", \"Gisele Bundchen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "latex_metadata": {
   "affiliation": "TU Delft",
   "author": "Georgios Gousios",
   "title": "Term relations with Word2Vec"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
